{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ef0e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a5b403",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90b35cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Images : 9500\n"
     ]
    }
   ],
   "source": [
    "data_dir=r\"C:\\Users\\speed\\Documents\\VALORANT Agents Classification TensorFlow CNN\\data\"\n",
    "train=[]\n",
    "classes=[i for i in os.listdir(data_dir)]\n",
    "for i in classes:\n",
    "    current_class=classes.index(i)\n",
    "    current_path=os.path.join(data_dir,i)\n",
    "    for j in os.listdir(current_path):\n",
    "        img=cv2.imread(os.path.join(data_dir,i,j),cv2.IMREAD_GRAYSCALE)\n",
    "        resized_img=cv2.resize(img,(70,70))\n",
    "        train.append([resized_img,current_class])\n",
    "# A Simple Algorithm To Multiply Images\n",
    "for i in range(4,501):\n",
    "    for j in os.listdir(r\"C:\\Users\\speed\\Documents\\VALORANT Agents Classification TensorFlow CNN\\data\"):\n",
    "        try:\n",
    "            shutil.copy(fr\"C:\\Users\\speed\\Documents\\VALORANT Agents Classification TensorFlow CNN\\data\\{j}\\Screenshot_3.png\",fr\"C:\\Users\\speed\\Documents\\VALORANT Agents Classification TensorFlow CNN\\data\\{j}\\Screenshot_{i}.png\")\n",
    "        except:\n",
    "            continue\n",
    "files=0\n",
    "for i,j,k in os.walk(r\"C:\\Users\\speed\\Documents\\VALORANT Agents Classification TensorFlow CNN\\data\"):\n",
    "    files+=len(k)\n",
    "print(f\"Num Images : {files}\")\n",
    "random.shuffle(train) # Shuffle Data To Avoid Overfitting\n",
    "x=[]\n",
    "y=[]\n",
    "for i,j in train:\n",
    "    x.append(i)\n",
    "    y.append(j)\n",
    "x=np.array(x)\n",
    "y=np.array(y)\n",
    "X=x/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371fe299",
   "metadata": {},
   "source": [
    "# Initialize TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9abcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsb=tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00efd114",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3c8edaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64,(3,3),activation=\"relu\",input_shape=(70,70,1)),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(64,(3,3),activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256,activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(128,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(len(classes),activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f9e254",
   "metadata": {},
   "source": [
    "# Compile The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "500345c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd826699",
   "metadata": {},
   "source": [
    "# Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1de5632",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "268/268 [==============================] - 62s 225ms/step - loss: 0.1584 - accuracy: 0.9602 - val_loss: 3.9304e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/30\n",
      "268/268 [==============================] - 60s 225ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 8.6192e-06 - val_accuracy: 1.0000\n",
      "Epoch 3/30\n",
      "268/268 [==============================] - 58s 218ms/step - loss: 6.7466e-05 - accuracy: 1.0000 - val_loss: 6.5251e-09 - val_accuracy: 1.0000\n",
      "Epoch 4/30\n",
      "268/268 [==============================] - 54s 202ms/step - loss: 2.3794e-05 - accuracy: 1.0000 - val_loss: 6.5251e-09 - val_accuracy: 1.0000\n",
      "Epoch 5/30\n",
      "268/268 [==============================] - 55s 206ms/step - loss: 6.3468e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 6/30\n",
      "268/268 [==============================] - 56s 210ms/step - loss: 5.5437e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 7/30\n",
      "268/268 [==============================] - 55s 204ms/step - loss: 3.9329e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 8/30\n",
      "268/268 [==============================] - 55s 203ms/step - loss: 5.2073e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 9/30\n",
      "268/268 [==============================] - 55s 204ms/step - loss: 6.2641e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 10/30\n",
      "268/268 [==============================] - 53s 199ms/step - loss: 0.0179 - accuracy: 0.9964 - val_loss: 5.1197e-08 - val_accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "268/268 [==============================] - 52s 195ms/step - loss: 8.9109e-04 - accuracy: 0.9998 - val_loss: 1.3427e-08 - val_accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "268/268 [==============================] - 52s 195ms/step - loss: 2.4038e-04 - accuracy: 0.9999 - val_loss: 3.3768e-07 - val_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "268/268 [==============================] - 53s 197ms/step - loss: 2.5125e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "268/268 [==============================] - 53s 197ms/step - loss: 1.2181e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "268/268 [==============================] - 52s 195ms/step - loss: 1.1898e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "268/268 [==============================] - 53s 196ms/step - loss: 2.0842e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "268/268 [==============================] - 53s 196ms/step - loss: 6.4414e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "268/268 [==============================] - 52s 196ms/step - loss: 1.8034e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "268/268 [==============================] - 51s 191ms/step - loss: 2.0668e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "268/268 [==============================] - 50s 187ms/step - loss: 6.5612e-07 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "268/268 [==============================] - 50s 188ms/step - loss: 1.1541e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "268/268 [==============================] - 50s 188ms/step - loss: 5.4870e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "268/268 [==============================] - 51s 190ms/step - loss: 2.9791e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "268/268 [==============================] - 50s 188ms/step - loss: 2.1959e-07 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "268/268 [==============================] - 50s 188ms/step - loss: 8.9956e-07 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "268/268 [==============================] - 50s 187ms/step - loss: 2.5823e-07 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "268/268 [==============================] - 51s 190ms/step - loss: 9.1851e-07 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "268/268 [==============================] - 58s 215ms/step - loss: 3.1138e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "268/268 [==============================] - 51s 191ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "268/268 [==============================] - 53s 198ms/step - loss: 2.8204e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1147d130f10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y,epochs=30,validation_split=0.1,callbacks=[tsb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a08b01",
   "metadata": {},
   "source": [
    "# Predict On Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a77b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(filepath):\n",
    "    img=cv2.imread(filepath,cv2.IMREAD_GRAYSCALE)\n",
    "    img=cv2.resize(img,(70,70))\n",
    "    return img.reshape(-1,70,70,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "58448d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'phoenix'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[np.argmax(model.predict(prepare_data(r\"C:\\Users\\speed\\Documents\\VALORANT Agents Classification TensorFlow CNN\\data\\phoenix\\Screenshot_19.png\")))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "910f667c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sova'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[np.argmax(model.predict(prepare_data(r\"C:\\Users\\speed\\Documents\\VALORANT Agents Classification TensorFlow CNN\\data\\sova\\Screenshot_19.png\")))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d130acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'viper'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[np.argmax(model.predict(prepare_data(r\"C:\\Users\\speed\\Documents\\VALORANT Agents Classification TensorFlow CNN\\data\\viper\\Screenshot_19.png\")))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a7cd9e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'chamber'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[np.argmax(model.predict(prepare_data(r\"C:\\Users\\speed\\Documents\\VALORANT Agents Classification TensorFlow CNN\\data\\chamber\\Screenshot_19.png\")))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da42f7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'breach'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[np.argmax(model.predict(prepare_data(r\"C:\\Users\\speed\\Documents\\VALORANT Agents Classification TensorFlow CNN\\data\\breach\\Screenshot_19.png\")))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3136736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yoru'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[np.argmax(model.predict(prepare_data(r\"C:\\Users\\speed\\Documents\\VALORANT Agents Classification TensorFlow CNN\\data\\yoru\\Screenshot_462.png\")))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55d3dff",
   "metadata": {},
   "source": [
    "# Save The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aece4a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd9616f",
   "metadata": {},
   "source": [
    "# Convert Model To TFLite Model For Edge Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6179e7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\speed\\AppData\\Local\\Temp\\tmpu1z8wgvb\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\speed\\AppData\\Local\\Temp\\tmpu1z8wgvb\\assets\n"
     ]
    }
   ],
   "source": [
    "tf_lite_converter=tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "with open(\"model.tflite\",\"wb\")as f:\n",
    "    f.write(tf_lite_converter.convert())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
